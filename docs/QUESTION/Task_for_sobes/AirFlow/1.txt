Задача 1. Перенос данных из PostgreSQL в Oracle по расписанию
Цель задачи
Реализовать ETL-процесс в Apache Airflow, который будет ежедневно извлекать данные из таблицы в PostgreSQL и загружать их в аналогичную таблицу в базе данных Oracle.

Источник и приёмник
Источник: база данны PostgreSQL
Подключение: Conn ID = postgres_source
Таблица: sales_staging (поля: id, product_name, quantity, price, sale_date, region)
Данные за предыдущий день (фильтр по sale_date >= yesterday AND sale_date < today)
Приёмник: база данных Oracle
Подключение: Conn ID = oracle_target
Таблица: SALES_DAILY (структура идентична sales_staging)
Режим загрузки: полная очистка и вставка (TRUNCATE + INSERT)

Требования:
-DAG должен запускаться ежедневно в 02:00.
-Используйте встроенные хуки Airflow: PostgresHook и OracleHook.
-Обеспечьте обработку ошибок (например, недоступности Oracle).
-При успешной загрузке — зарегистрировать количество вставленных строк.
-DAG должен быть идемпотентным (при правильной реализации повторный запуск не должен приводить к дублированию данных).

Ожидаемый результат:
-DAG с именем postgres_to_oracle_etl.
-Один основной оператор Python, выполняющий:
-Чтение данных из PostgreSQL.
-Преобразование (при необходимости — приведение типов).
-Очистка целевой таблицы в Oracle.
-Вставка данных.
-Логи показывают количество обработанных строк.
-Поддержка Airflow 2.x и провайдеров (apache-airflow-providers-oracle, apache-airflow-providers-postgres).

Критерии успеха:
-DAG работает по расписанию
-Данные корректно переносятся
-Обработка ошибок и логирование на месте
-Используются хуки, а не «ручные» подключения через psycopg2/cx_Oracle напрямую